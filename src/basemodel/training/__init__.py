"""
è®­ç»ƒæ¨¡å—

æä¾› TRPG NER æ¨¡å‹è®­ç»ƒåŠŸèƒ½ã€‚

æ³¨æ„: ä½¿ç”¨æ­¤æ¨¡å—éœ€è¦å®‰è£…è®­ç»ƒä¾èµ–:
    pip install base-model-trpgner[train]
"""

import os
from typing import Optional, List
from pathlib import Path


def train_ner_model(
    conll_data: str,
    model_name_or_path: str = "hfl/minirbt-h256",
    output_dir: str = "./models/trpg-ner-v1",
    num_train_epochs: int = 20,
    per_device_train_batch_size: int = 4,
    learning_rate: float = 5e-5,
    max_length: int = 128,
    resume_from_checkpoint: Optional[str] = None,
) -> None:
    """
    è®­ç»ƒ NER æ¨¡å‹

    Args:
        conll_data: CoNLL æ ¼å¼æ•°æ®æ–‡ä»¶æˆ–ç›®å½•
        model_name_or_path: åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„
        output_dir: æ¨¡å‹è¾“å‡ºç›®å½•
        num_train_epochs: è®­ç»ƒè½®æ•°
        per_device_train_batch_size: æ‰¹å¤„ç†å¤§å°
        learning_rate: å­¦ä¹ ç‡
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
        resume_from_checkpoint: æ¢å¤æ£€æŸ¥ç‚¹è·¯å¾„

    Examples:
        >>> from basemodel.training import train_ner_model
        >>> train_ner_model(
        ...     conll_data="./data",
        ...     output_dir="./my_model",
        ...     epochs=10
        ... )
    """
    try:
        import torch
        from transformers import (
            AutoTokenizer,
            AutoModelForTokenClassification,
            TrainingArguments,
            Trainer,
        )
        from datasets import Dataset
        from tqdm.auto import tqdm
    except ImportError as e:
        raise ImportError(
            "è®­ç»ƒä¾èµ–æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install base-model-trpgner[train]"
        ) from e

    # å¯¼å…¥æ•°æ®å¤„ç†å‡½æ•°
    from basemodel.utils.conll import load_conll_dataset, tokenize_and_align_labels

    print(f"ğŸš€ Starting training...")

    # åŠ è½½æ•°æ®
    dataset, label_list = load_conll_dataset(conll_data)
    label2id = {label: i for i, label in enumerate(label_list)}
    id2label = {i: label for i, label in enumerate(label_list)}

    # åˆå§‹åŒ–æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
    if tokenizer.model_max_length > 1000:
        tokenizer.model_max_length = max_length

    model = AutoModelForTokenClassification.from_pretrained(
        model_name_or_path,
        num_labels=len(label_list),
        id2label=id2label,
        label2id=label2id,
        ignore_mismatched_sizes=True,
    )

    # Tokenize
    tokenized_dataset = dataset.map(
        lambda ex: tokenize_and_align_labels(ex, tokenizer, label2id, max_length),
        batched=True,
        remove_columns=["text", "char_labels"],
    )

    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        learning_rate=learning_rate,
        per_device_train_batch_size=per_device_train_batch_size,
        num_train_epochs=num_train_epochs,
        logging_steps=5,
        save_steps=200,
        save_total_limit=2,
        do_eval=False,
        report_to="none",
        no_cuda=not torch.cuda.is_available(),
        load_best_model_at_end=False,
        push_to_hub=False,
        fp16=torch.cuda.is_available(),
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
    )

    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ Starting training...")
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)

    # ä¿å­˜æ¨¡å‹
    print("ğŸ’¾ Saving final model...")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)

    print(f"âœ… Training finished. Model saved to {output_dir}")


def export_to_onnx(
    model_dir: str,
    onnx_path: str,
    max_length: int = 128,
) -> bool:
    """
    å°†è®­ç»ƒå¥½çš„æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼

    Args:
        model_dir: æ¨¡å‹ç›®å½•
        onnx_path: ONNX è¾“å‡ºè·¯å¾„
        max_length: æœ€å¤§åºåˆ—é•¿åº¦

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        import torch
        from torch.onnx import export as onnx_export
        from transformers import AutoTokenizer, AutoModelForTokenClassification
        import onnx
    except ImportError as e:
        raise ImportError(
            "ONNX å¯¼å‡ºä¾èµ–æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install onnx"
        ) from e

    print(f"ğŸ“¤ Exporting model from {model_dir} to {onnx_path}...")

    model_dir = os.path.abspath(model_dir)
    if not os.path.exists(model_dir):
        raise FileNotFoundError(f"Model directory not found: {model_dir}")

    # åŠ è½½æ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)
    model = AutoModelForTokenClassification.from_pretrained(
        model_dir, local_files_only=True
    )
    model.eval()

    # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
    dummy_text = "èè 2024-06-08 21:46:26"
    inputs = tokenizer(
        dummy_text,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
        max_length=max_length,
    )

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(onnx_path), exist_ok=True)

    # å¯¼å‡º ONNX
    onnx_export(
        model,
        (inputs["input_ids"], inputs["attention_mask"]),
        onnx_path,
        export_params=True,
        opset_version=18,
        do_constant_folding=True,
        input_names=["input_ids", "attention_mask"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch_size", 1: "sequence_length"},
            "attention_mask": {0: "batch_size", 1: "sequence_length"},
            "logits": {0: "batch_size", 1: "sequence_length"},
        },
    )

    # éªŒè¯ ONNX æ¨¡å‹
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)

    size_mb = os.path.getsize(onnx_path) / 1024 / 1024
    print(f"âœ… ONNX export successful! Size: {size_mb:.2f} MB")
    return True


__all__ = ["train_ner_model", "export_to_onnx"]
